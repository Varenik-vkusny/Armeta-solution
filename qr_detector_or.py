import cv2
import ultralytics
from pyzbar.pyzbar import decode
import numpy as np
from ultralytics import YOLO
from concurrent.futures import ThreadPoolExecutor
import time

MODEL_PATH = "best.pt"
try:
    yolo_model = YOLO(MODEL_PATH)
    print(f"--- –ú–æ–¥–µ–ª—å YOLO ('{MODEL_PATH}') —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. ---")
except Exception as e:
    print(
        f"--- [–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å YOLO '{MODEL_PATH}'. –û—à–∏–±–∫–∞: {e} ---"
    )
    yolo_model = None


def _run_pyzbar_fast(image_to_scan, methods=["grayscale", "adaptive_thresh"]):
    """
    [–£–õ–¨–¢–†–ê-–ë–´–°–¢–†–ê–Ø –í–ï–†–°–ò–Ø]
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã Pyzbar.
    –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: grayscale + adaptive_thresh (–æ–Ω–∏ –Ω–∞—Ö–æ–¥—è—Ç 90%+ –≤—Å–µ—Ö QR)
    """
    found_objects = []
    gray = cv2.cvtColor(image_to_scan, cv2.COLOR_BGR2GRAY)

    all_methods = {
        "grayscale": gray,
        "adaptive_thresh": None,
        "otsu_thresh": None,
        "original_bgr": image_to_scan,
    }

    for method_name in methods:
        if method_name not in all_methods:
            continue

        if method_name == "adaptive_thresh" and all_methods[method_name] is None:
            blurred = cv2.GaussianBlur(gray, (3, 3), 0)
            all_methods[method_name] = cv2.adaptiveThreshold(
                blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 7
            )
        elif method_name == "otsu_thresh" and all_methods[method_name] is None:
            blurred = cv2.GaussianBlur(gray, (3, 3), 0)
            all_methods[method_name] = cv2.threshold(
                blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU
            )[1]

        image_for_decode = all_methods[method_name]

        try:
            qrcodes = decode(image_for_decode)
            for qr in qrcodes:
                found_objects.append({"qr_obj": qr, "source": f"pyzbar_{method_name}"})
        except Exception as e:
            pass

    return found_objects


def _process_scale(scale, input_image, orig_w, orig_h):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏)
    """
    if scale == 1.0:
        scaled_image = input_image
    else:
        try:
            interpolation_method = cv2.INTER_AREA if scale < 1.0 else cv2.INTER_CUBIC
            scaled_image = cv2.resize(
                input_image,
                (int(orig_w * scale), int(orig_h * scale)),
                interpolation=interpolation_method,
            )
        except Exception:
            return []

    if scaled_image is None or scaled_image.size == 0:
        return []

    found_objects = _run_pyzbar_fast(
        scaled_image, methods=["grayscale", "adaptive_thresh"]
    )

    results = []
    for item in found_objects:
        qr_obj = item["qr_obj"]
        source = item["source"]
        data_bytes = qr_obj.data
        points_scaled = np.array(qr_obj.polygon, dtype=np.float32)

        if data_bytes is not None:
            points_original = points_scaled / scale
            results.append(
                {
                    "data": data_bytes,
                    "points": points_original,
                    "source": f"{source}_scale_{scale}x",
                }
            )

    return results


def is_duplicate(new_points, existing_qrs, threshold=0.7):
    """
    [–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û] –ü–æ—Ä–æ–≥ —Å–Ω–∏–∂–µ–Ω –¥–æ 0.7 –¥–ª—è –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    """
    new_box = cv2.boundingRect(new_points.astype(int))
    nx, ny, nw, nh = new_box

    for qr in existing_qrs:
        existing_box = cv2.boundingRect(qr["points"].astype(int))
        ex, ey, ew, eh = existing_box

        if nx + nw < ex or ex + ew < nx or ny + nh < ey or ey + eh < ny:
            continue

        ix = max(nx, ex)
        iy = max(ny, ey)
        iw = min(nx + nw, ex + ew) - ix
        ih = min(ny + nh, ey + eh) - iy

        if iw > 0 and ih > 0:
            intersection_area = iw * ih
            union_area = (nw * nh) + (ew * eh) - intersection_area
            iou = intersection_area / union_area

            if iou > threshold:
                return True

    return False


def add_qr_code_detections_turbo(
    input_image_np,
    existing_page_data,
    annotation_start_index: int,
    use_parallel=True,
    early_stop=True,
):
    """
    üöÄ –¢–£–†–ë–û-–í–ï–†–°–ò–Ø: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - use_parallel: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –º–∞—Å—à—Ç–∞–±–æ–≤ (–±—ã—Å—Ç—Ä–µ–µ –Ω–∞ –º–æ—â–Ω—ã—Ö CPU)
    - early_stop: –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –ø–æ—Å–ª–µ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è QR-–∫–æ–¥–æ–≤ (—ç–∫–æ–Ω–æ–º–∏—Ç –≤—Ä–µ–º—è)
    """
    start_time = time.time()
    print("--- üöÄ –¢–£–†–ë–û-–î–ï–¢–ï–ö–¢–û–†: –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ QR-–∫–æ–¥–æ–≤...")

    found_qrs = []
    image_with_boxes = input_image_np.copy()
    current_annotation_index = annotation_start_index
    orig_h, orig_w, _ = input_image_np.shape

    print("--- [–°—Ç—Ä–∞—Ç–µ–≥–∏—è 1] –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞—Ç–∏–≤–Ω–æ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏...")

    native_results = _process_scale(1.0, input_image_np, orig_w, orig_h)
    for item in native_results:
        if not is_duplicate(item["points"], found_qrs):
            found_qrs.append(item)

    print(f"    –ù–∞–π–¥–µ–Ω–æ: {len(found_qrs)} QR-–∫–æ–¥–æ–≤")

    if early_stop and len(found_qrs) > 0:
        print("    ‚úì QR-–∫–æ–¥—ã –Ω–∞–π–¥–µ–Ω—ã! –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Å—à—Ç–∞–±—ã.")
    else:
        print("--- [–°—Ç—Ä–∞—Ç–µ–≥–∏—è 2] –ú—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω—ã–π –ø–æ–∏—Å–∫...")

        additional_scales = [0.5, 2.0]

        if use_parallel:
            with ThreadPoolExecutor(max_workers=2) as executor:
                futures = [
                    executor.submit(
                        _process_scale, scale, input_image_np, orig_w, orig_h
                    )
                    for scale in additional_scales
                ]

                for future in futures:
                    try:
                        results = future.result(timeout=5.0)
                        for item in results:
                            if not is_duplicate(item["points"], found_qrs):
                                found_qrs.append(item)
                    except Exception as e:
                        print(f"    ‚ö† –û—à–∏–±–∫–∞ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        else:
            for scale in additional_scales:
                results = _process_scale(scale, input_image_np, orig_w, orig_h)
                for item in results:
                    if not is_duplicate(item["points"], found_qrs):
                        found_qrs.append(item)

        print(f"    –ù–∞–π–¥–µ–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: {len(found_qrs)} QR-–∫–æ–¥–æ–≤ (–≤—Å–µ–≥–æ)")

    if yolo_model and len(found_qrs) == 0:
        print("--- [–°—Ç—Ä–∞—Ç–µ–≥–∏—è 3] Pyzbar –Ω–µ –Ω–∞—à–µ–ª - –ø—Ä–æ–±—É–µ–º YOLO...")
        try:
            results = yolo_model(input_image_np, conf=0.40, verbose=False, imgsz=640)
            boxes = results[0].boxes.xyxy.cpu().numpy()

            if len(boxes) > 0:
                print(f"    YOLO –Ω–∞—à–µ–ª {len(boxes)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤...")

                for box in boxes:
                    x1, y1, x2, y2 = map(int, box)
                    box_w = x2 - x1
                    box_h = y2 - y1

                    pad_x = int(box_w * 0.10)
                    pad_y = int(box_h * 0.10)
                    y_start = max(0, y1 - pad_y)
                    y_end = min(orig_h, y2 + pad_y)
                    x_start = max(0, x1 - pad_x)
                    x_end = min(orig_w, x2 + pad_x)

                    qr_crop = input_image_np[y_start:y_end, x_start:x_end]

                    if qr_crop.size == 0:
                        continue

                    decoded_objects = _run_pyzbar_fast(qr_crop, methods=["grayscale"])

                    if decoded_objects:
                        for item in decoded_objects:
                            qr_obj = item["qr_obj"]
                            source = item["source"]
                            data_bytes = qr_obj.data
                            points_crop = np.array(qr_obj.polygon, dtype=np.float32)

                            if data_bytes is not None:
                                points_original = points_crop + [x_start, y_start]

                                if not is_duplicate(points_original, found_qrs):
                                    found_qrs.append(
                                        {
                                            "data": data_bytes,
                                            "points": points_original,
                                            "source": f"yolo_confirmed_by_{source}",
                                        }
                                    )
                                    print("    ‚úì YOLO –Ω–∞—à–µ–ª –Ω–æ–≤—ã–π QR!")
                                    break

        except Exception as e:
            print(f"    ‚ö† –û—à–∏–±–∫–∞ YOLO: {e}")

    if found_qrs:
        print(f"--- ‚úì –ù–∞–π–¥–µ–Ω–æ {len(found_qrs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö QR-–∫–æ–¥–æ–≤")
        for item in found_qrs:
            points = item["points"]
            source = item["source"]

            x, y, w, h = cv2.boundingRect(points.astype(int))

            annotation = {
                f"annotation_{current_annotation_index}": {
                    "category": "qr_code",
                    "bbox": {"x": x, "y": y, "width": w, "height": h},
                    "area": w * h,
                }
            }
            existing_page_data["annotations"].append(annotation)

            pts = points.astype(np.int32).reshape((-1, 1, 2))
            cv2.polylines(
                image_with_boxes, [pts], isClosed=True, color=(0, 255, 0), thickness=3
            )
            cv2.putText(
                image_with_boxes,
                f"QR",
                (x, y - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.9,
                (0, 255, 0),
                2,
            )
            current_annotation_index += 1
    else:
        print("--- QR-–∫–æ–¥—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    elapsed = time.time() - start_time
    print(f"--- ‚è± –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {elapsed:.2f}s")

    return image_with_boxes, existing_page_data, current_annotation_index


add_qr_code_detections_ultimate = add_qr_code_detections_turbo
